---
title: "Word2Vec implementation with Python"
layout: post
date: 2020-07-22
tags: blog
comments: true
---
###### Goal
* To demonstrate a simple implementation of Word2Vec using gensim and python3.

###### Import dependencies
{% highlight python %}
import pandas as pd
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
{% endhighlight %}

###### Load the data
Download the dataset: [6 text Commentaries for batsman Steve Smith](https://swarup-rj.github.io/assets/data/SmithCommentary.csv)
{% highlight python %}
data = pd.read_csv('SmithCommentary.csv', names = ['commentaries'])
{% endhighlight %}

{% highlight python %}
>>> pprint(data)
                                        commentaries
0  gets another no ball from Gul first up  apart ...
1  this is better  in the sense that it was a leg...
2  this was a touch wide again  but not as wide a...
3  gets the line right  perhaps its a touch too s...
4  well judged by Smith  who looks confident enou...
5  Smith is away  working this to midwicket and s...
{% endhighlight %}

###### Process the data
{% highlight python %}
#Make a list of commentaries
commentaries = list(data['commentaries'])
#Lower the upper case letters. Also perform other text preprocessing (stop words, stemming, etc.) if required.
commentaries = [commentary.lower() for commentary in commentaries]
##Convert to list of list format (required for input to word2vec in gensim).
list_of_list = [word_tokenize(commentary) for commentary in commentaries]
{% endhighlight %}

{% highlight python %}
>>> print(list_of_list)
[['gets', 'another', 'no', 'ball', 'from', 'gul', 'first', 'up', 'apart', 'from', 'that', 'it', 'was', 'a', 'good', 'delivery', 'yorker', 'that', 'smith', 'just', 'jabbed', 'a', 'bat', 'down', 'in', 'time', 'on'], ['this', 'is', 'better', 'in', 'the', 'sense', 'that', 'it', 'was', 'a', 'legal', 'delivery', 'but', 'worse', 'in', 'terms', 'of', 'line', 'giving', 'smith', 'an', 'early', 'chance', 'to', 'leave', 'alone', 'outside', 'off'], ['this', 'was', 'a', 'touch', 'wide', 'again', 'but', 'not', 'as', 'wide', 'as', 'gul', 'smile', 'after', 'he', 'saw', 'this', 'bend', 'away', 'late', 'big', 'reverse', 'swing', 'which', 'might', 'keep', 'things', 'interesting'], ['gets', 'the', 'line', 'right', 'perhaps', 'its', 'a', 'touch', 'too', 'short', 'though', 'and', 'smith', 'gets', 'behind', 'a', 'solid', 'defence'], ['well', 'judged', 'by', 'smith', 'who', 'looks', 'confident', 'enough', 'leaving', 'alone'], ['smith', 'is', 'away', 'working', 'this', 'to', 'midwicket', 'and', 'scampering', 'through', 'for', 'a', 'single']]
{% endhighlight %}

###### Train the Word2Vec model
Parameters:
1. Data: list_of_lists
2. min_count: minimum count of words (default: 5)
3. size: size of the embedding vector (default: 100)
4. workers: number of partitions (default: 3)
4. window: maximum distance between a target word and words around it (default: 5)

{% highlight python %}
model = Word2Vec(list_of_list, min_count=1,size= 50,workers=3, window =4)
{% endhighlight %}

###### Check word embedding
{% highlight python %}
embedding = model['defence']
{% endhighlight %}

{% highlight python %}
>>> print(embedding)
[ 9.5988094e-04  5.9676549e-04 -3.1163890e-03  6.5261363e-03
 -1.7907316e-04 -4.9430579e-03 -2.2877546e-03 -5.8922358e-03
 -3.6874264e-03 -1.5599853e-03 -6.0183038e-03 -3.1733824e-04
 -5.2389144e-03  1.0354079e-03 -8.4035471e-03  4.3296660e-03
 -4.8308210e-03 -9.9479537e-03 -8.1190519e-05 -7.0892340e-03
 -3.8349126e-03  1.2542350e-03 -1.2487450e-03 -3.3740092e-03
 -9.6818721e-03  8.9819832e-03 -9.3914568e-03  4.6300814e-03
 -6.2629483e-03  4.5282487e-03  6.2015434e-03  8.5394634e-03
 -5.1377648e-03 -4.6001440e-03  7.6431953e-03  5.7597305e-03
 -8.0888430e-03 -5.6318371e-03  7.1958178e-03 -6.3757305e-03
  6.2486399e-03 -3.4614438e-03  6.5972065e-03  7.7094031e-03
  7.8785438e-03  2.5848555e-03 -6.6702664e-03  9.5386980e-03
  4.1816410e-04  4.9292687e-03]
{% endhighlight %}

###### Find similarity between words
{% highlight python %}
similarity = model.similarity('defence', 'leave')
{% endhighlight %}

{% highlight python %}
>>> print(similarity)
0.20050177
{% endhighlight %}

###### Find most similar words
{% highlight python %}
most_similar = model.most_similar('defence')
{% endhighlight %}

{% highlight python %}
>>> print(most_similar)
[('bat', 0.3753732144832611), ('line', 0.33381927013397217), ('on', 0.2843920886516571), ('just', 0.2544698119163513), ('legal', 0.24942444264888763), ('yorker', 0.22119580209255219), ('leave', 0.20050175487995148), ('reverse', 0.19984550774097443), ('that', 0.1919858455657959), ('not', 0.18695269525051117)]
{% endhighlight %}


###### Final Code
{% highlight python %}
#Import dependencies
import pandas as pd
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize

#Data loading and preprocessing
data = pd.read_csv('SmithCommentary.csv', names = ['commentaries'])
commentaries = list(data['commentaries'])
commentaries = [commentary.lower() for commentary in commentaries]
list_of_list = [word_tokenize(commentary) for commentary in commentaries]

#Train the Word2Vec model
model = Word2Vec(list_of_list, min_count=1,size= 50,workers=3, window =4)

#Check word embedding
embedding = model['defence']
print(embedding)

#Check word similarity
similarity = model.similarity('defence', 'leave')
print(similarity)

most_similar = model.most_similar('defence')
print(most_similar)
{% endhighlight %}
