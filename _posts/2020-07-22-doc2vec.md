---
title: "Doc2Vec implementation with Python"
layout: post
date: 2020-07-22
tags: blog
comments: true
---
###### Goal
* To demonstrate a simple implementation of Doc2Vec using gensim and python3.

###### Import dependencies
{% highlight python %}
import pandas as pd
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
{% endhighlight %}

###### Load the data
Download the dataset: [6 text Commentaries for batsman Steve Smith](https://swarup-rj.github.io/assets/data/SmithCommentary.csv)
{% highlight python %}
data = pd.read_csv('SmithCommentary.csv', names = ['commentaries'])
{% endhighlight %}

{% highlight python %}
>>> pprint(data)
                                        commentaries
0  gets another no ball from Gul first up  apart ...
1  this is better  in the sense that it was a leg...
2  this was a touch wide again  but not as wide a...
3  gets the line right  perhaps its a touch too s...
4  well judged by Smith  who looks confident enou...
5  Smith is away  working this to midwicket and s...
{% endhighlight %}

###### Process the data
{% highlight python %}
#Make a list of commentaries
commentaries = list(data['commentaries'])
#Lower the upper case letters. Also perform other text preprocessing (stop words, stemming, etc.) if required.
commentaries = [commentary.lower() for commentary in commentaries]
##Convert to tagged_data format (required for input to doc2vec in gensim).
tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(commentaries)]
{% endhighlight %}

{% highlight python %}
>>> print(tagged_data)
[TaggedDocument(words=['gets', 'another', 'no', 'ball', 'from', 'gul', 'first', 'up', 'apart', 'from', 'that', 'it', 'was', 'a', 'good', 'delivery', 'yorker', 'that', 'smith', 'just', 'jabbed', 'a', 'bat', 'down', 'in', 'time', 'on'], tags=['0']), TaggedDocument(words=['this', 'is', 'better', 'in', 'the', 'sense', 'that', 'it', 'was', 'a', 'legal', 'delivery', 'but', 'worse', 'in', 'terms', 'of', 'line', 'giving', 'smith', 'an', 'early', 'chance', 'to', 'leave', 'alone', 'outside', 'off'], tags=['1']), TaggedDocument(words=['this', 'was', 'a', 'touch', 'wide', 'again', 'but', 'not', 'as', 'wide', 'as', 'gul', 'smile', 'after', 'he', 'saw', 'this', 'bend', 'away', 'late', 'big', 'reverse', 'swing', 'which', 'might', 'keep', 'things', 'interesting'], tags=['2']), TaggedDocument(words=['gets', 'the', 'line', 'right', 'perhaps', 'its', 'a', 'touch', 'too', 'short', 'though', 'and', 'smith', 'gets', 'behind', 'a', 'solid', 'defence'], tags=['3']), TaggedDocument(words=['well', 'judged', 'by', 'smith', 'who', 'looks', 'confident', 'enough', 'leaving', 'alone'], tags=['4']), TaggedDocument(words=['smith', 'is', 'away', 'working', 'this', 'to', 'midwicket', 'and', 'scampering', 'through', 'for', 'a', 'single'], tags=['5'])]
{% endhighlight %}

###### Train the Doc2Vec model
{% highlight python %}
max_epochs = 100
vec_size = 20
alpha = 0.025
model = Doc2Vec(size=vec_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm =1)
model.build_vocab(tagged_data)
for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.iter)
    model.alpha -= 0.0002
    model.min_alpha = model.alpha
{% endhighlight %}

###### Save the trained Doc2Vec model
{% highlight python %}
model.save("doc2vec.model")
{% endhighlight %}

###### Load the trained Doc2Vec model
{% highlight python %}
model= Doc2Vec.load("doc2vec.model")
{% endhighlight %}


###### Find most similar documents using TAGs
{% highlight python %}
similar_doc = model.docvecs.most_similar('1')
{% endhighlight %}

{% highlight python %}
>>> print(similar_doc)
[('5', 0.5658907294273376), ('3', 0.4622122645378113), ('0', 0.38878512382507324), ('4', 0.351872980594635), ('2', 0.14253216981887817)]
{% endhighlight %}

###### Find TOP-N most similar documents using TAGs
{% highlight python %}
similar_doc = model.docvecs.most_similar('1',topn=3)
{% endhighlight %}

{% highlight python %}
>>> print(similar_doc)
[('5', 0.5658907294273376), ('3', 0.4622122645378113), ('0', 0.38878512382507324)]
{% endhighlight %}


###### Find the vector of a new document
{% highlight python %}
test = word_tokenize("short of a length, he defended".lower())
vector = model.infer_vector(test)
{% endhighlight %}

{% highlight python %}
>>> print("vector: ", vector)
('vector: ', array([ 0.01889105, -0.01043655,  0.01137873,  0.00385578,  0.00474158,
       -0.0157043 , -0.0059559 ,  0.00478037,  0.01172048,  0.00031838,
        0.00750044, -0.01989358,  0.00825847,  0.01239266, -0.003056  ,
        0.01539477,  0.01089285,  0.00233861, -0.01773974,  0.03058155],
      dtype=float32))
{% endhighlight %}



###### Final Code
{% highlight python %}
import pandas as pd
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
data = pd.read_csv('SmithCommentary.csv', names = ['commentaries'])
commentaries = list(data['commentaries'])
tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(commentaries)]

max_epochs = 100
vec_size = 20
alpha = 0.025
model = Doc2Vec(size=vec_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm =1)
model.build_vocab(tagged_data)
for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.iter)
    model.alpha -= 0.0002
    model.min_alpha = model.alpha

model.save("d2v.model")
model= Doc2Vec.load("d2v.model")

#To find similar documents
similar_doc = model.docvecs.most_similar('1')
print(similar_doc)

similar_doc = model.docvecs.most_similar('6',topn=150)
print(similar_doc)

#To find the vector of a new document
test = word_tokenize("short of a length, he defended".lower())
vector = model.infer_vector(test)
print("vector: ", vector)
{% endhighlight %}
